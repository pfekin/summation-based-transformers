# High-Dimensional Aggregation for Efficient Sequence Modeling  

[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)  

> **Sub-quadratic sequence modeling** through high-dimensional embeddings and geometric orthogonality. Matches attention performance with **7â€“20Ã— faster computation**.

---

## ðŸš€ Key Features  
- **Linear complexity** O(nÂ·d) vs. O(nÂ²Â·d)
- **Position-semantic fusion** via element-wise multiplication  
- **Emergent orthogonality**: Self-organized token embeddings  
- **Derrida-aligned latent space**: Meaning through relational contrasts  

---

## ðŸ“Š Performance Highlights  

### Cosine Similarity Distribution (Aggregation vs. Attention)  
![Cosine Similarity](media/histogram.png)  
*Token embedding orthogonality emerges naturally*

---

## âš™ï¸ Core Mechanics  

### Position-Semantic Fusion  
Combines token embeddings (**eâ‚œ**) and positional encodings (**pâ‚œ**) via:  
**ContextVector** = Î£â‚œâ‚Œâ‚â¿ (eâ‚œ âŠ™ pâ‚œ)  
- **âŠ™**: Element-wise multiplication  
- Preserves both position and semantics through multiplicative interaction  


### Training Dynamics  
The optimizer achieves dual objectives:  
1. **Minimizes semantic clash** through emergent orthogonality  
2. **Maintains positional fidelity** via element multiplication  

---

## ðŸŒ Philosophical Alignment  

| Concept               | Implementation                   | 
|-----------------------|-----------------------------------|
| Derrida's *diffÃ©rance*| Meaning through vector contrasts |
| Atlan's crystal-smoke | Orthogonal structure + training chaos |
| Deleuze's multiplicity| Superposition without collision   |

---

## ðŸ§ª Benchmark Results  

| Task            | Aggregation | Attention | Speedup |  
|-----------------|-------------|-----------|---------|  
| Reuters Classification (Validation Accuracy) | 78.63%      | 77.96%     | 20.0Ã—    |  
| AG News Autoregression (Validation Perplexity) | 2.99  (less is better)    | 3.15     | 1.5Ã—     |  
---

## ðŸ› ï¸ Implementation  
```python  

# Aggregation
class Aggregation(layers.Layer):
    def __init__(self, d_model, projection=True, noise_stddev=0.0):
        super().__init__()
        self.d_model = d_model
        self.projection = projection
        self.proj = layers.Dense(d_model, activation='linear', use_bias=False) if projection is not None else None
        self.noise = layers.GaussianNoise(noise_stddev, seed=None)
        
    def call(self, x, mask=None, training=None):
        x = self.proj(x) if self.projection else x
             
        if mask is not None:
            # Convert mask to float32 and use it to zero out future positions
            mask = tf.cast(mask, tf.float32)
            
            # Aggregate using matrix multiplication (avoids 4D tensor)
            x = tf.einsum('bij,bjf->bif', mask, x)  # (batch, seq_len, d_model*expansion)
        else:
            x = tf.reduce_sum(x, axis=1, keepdims=True)
            
        x = self.noise(x, training=training)
        return x  # (batch, seq_len, d_model)
```
## ðŸ“š Future Directions  
1. Embedding dimension vs. sequence length tradeoffs  
2. Extending to multimodal inputs (text, vision, audio)
3. Use in a diffusion model

---

## ðŸ“¦ Installation  
```bash  
git clone https://github.com/yourusername/high-dimensional-aggregation  
pip install -r requirements.txt  
```
## ðŸ“œ License  
MIT Â© Pascal Ekin. See [LICENSE](LICENSE) for details.  	
